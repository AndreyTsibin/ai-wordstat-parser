#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import sys
import urllib.request
import urllib.error
import time


# API –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
API_BASE_URL = "https://api.wordstat.yandex.net"
API_METHOD = "/v1/topRequests"

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
CATEGORY_KEYWORDS = {
    'commercial': [
        '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–ø–æ–¥ –∫–ª—é—á', '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ',
        '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–∞–π—Å', '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞',
        '–¥–æ—Å—Ç–∞–≤–∫–∞', '—É—Å—Ç–∞–Ω–æ–≤–∫–∞', '–º–æ–Ω—Ç–∞–∂', '–≤—ã–∑–≤–∞—Ç—å', '—É—Å–ª—É–≥–∏'
    ],
    'informational': [
        '–∫–∞–∫', '—á—Ç–æ —Ç–∞–∫–æ–µ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–æ–≥–¥–∞',
        '–≥–¥–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—Å–ø–æ—Å–æ–±—ã',
        '–º–µ—Ç–æ–¥—ã', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '—Å–æ–≤–µ—Ç—ã', '—ç—Ç–∞–ø—ã'
    ],
    'price': [
        '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø—Ä–∞–π—Å', '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç',
        '—Ä–∞—Å—Ü–µ–Ω–∫–∏', '—Ç–∞—Ä–∏—Ñ', '—Å—Ç–æ–∏—Ç', '–∑–∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –º–µ—Ç—Ä'
    ],
    'comparison': [
        '–æ—Ç–∑—ã–≤—ã', '—Ä–µ–π—Ç–∏–Ω–≥', '–ª—É—á—à–∏–µ', '—Ç–æ–ø', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ',
        'vs', '–∏–ª–∏', '–∫–∞–∫–æ–π –≤—ã–±—Ä–∞—Ç—å', '—á—Ç–æ –ª—É—á—à–µ'
    ]
}

# –≠–º–æ–¥–∑–∏ –¥–ª—è —Ç–∏–ø–æ–≤
CATEGORY_EMOJI = {
    'commercial': 'üõí',
    'informational': 'üìö',
    'price': 'üí∞',
    'comparison': '‚öñÔ∏è',
    'local': 'üìç',
    'other': 'üîç'
}


def load_env():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞ .env"""
    env_path = '.env'

    if not os.path.exists(env_path):
        print("‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üìù –°–∫–æ–ø–∏—Ä—É–π—Ç–µ .env.example –≤ .env –∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω")
        sys.exit(1)

    with open(env_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()

    token = os.getenv('YANDEX_WORDSTAT_TOKEN')
    if not token or token == 'your_token_here':
        print("‚ùå –û—à–∏–±–∫–∞: —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .env")
        sys.exit(1)

    return token


def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.json"""
    config_path = 'config.json'

    if not os.path.exists(config_path):
        print("‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª config.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        sys.exit(1)

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ config.json: {e}")
        sys.exit(1)

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    required_fields = ['business_info', 'parser_settings']
    for field in required_fields:
        if field not in config:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ø–æ–ª–µ '{field}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ config.json")
            sys.exit(1)

    return config


def load_queries():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ queries.txt"""
    queries_path = 'queries.txt'

    if not os.path.exists(queries_path):
        print("‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª queries.txt –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üìù –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Claude Code –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤")
        sys.exit(1)

    with open(queries_path, 'r', encoding='utf-8') as f:
        queries = [line.strip() for line in f if line.strip()]

    if not queries:
        print("‚ùå –û—à–∏–±–∫–∞: queries.txt –ø—É—Å—Ç–æ–π!")
        sys.exit(1)

    return queries


def fetch_top_requests(token, phrase, region, devices, max_retries=3):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ñ—Ä–∞–∑—ã –∏–∑ API –í–æ—Ä–¥—Å—Ç–∞—Ç–∞

    Args:
        token: OAuth —Ç–æ–∫–µ–Ω
        phrase: –ü–æ–∏—Å–∫–æ–≤–∞—è —Ñ—Ä–∞–∑–∞
        region: –ö–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 213 –¥–ª—è –ú–æ—Å–∫–≤—ã)
        devices: –°–ø–∏—Å–æ–∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ ['desktop', 'mobile']
        max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ

    Returns:
        dict: JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    url = API_BASE_URL + API_METHOD

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞
    data = {
        "phrase": phrase,
        "regions": [region],
        "devices": devices
    }

    json_data = json.dumps(data).encode('utf-8')

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = {
        'Content-Type': 'application/json;charset=utf-8',
        'Authorization': f'Bearer {token}'
    }

    # –ü–æ–ø—ã—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ —Å —Ä–µ—Ç—Ä–∞—è–º–∏
    for attempt in range(1, max_retries + 1):
        try:
            request = urllib.request.Request(
                url,
                data=json_data,
                headers=headers,
                method='POST'
            )

            with urllib.request.urlopen(request, timeout=30) as response:
                result = json.loads(response.read().decode('utf-8'))
                return result

        except urllib.error.HTTPError as e:
            error_body = e.read().decode('utf-8') if e.fp else 'No details'
            print(f"   ‚ö†Ô∏è HTTP –æ—à–∏–±–∫–∞ {e.code}: {error_body}")

            if attempt < max_retries:
                wait_time = 5 * attempt
                print(f"   üîÑ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫... (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                return None

        except urllib.error.URLError as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e.reason}")

            if attempt < max_retries:
                wait_time = 5 * attempt
                print(f"   üîÑ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫...")
                time.sleep(wait_time)
            else:
                return None

        except Exception as e:
            print(f"   ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return None

    return None


def has_minus_words(phrase, minus_words):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ñ—Ä–∞–∑–∞ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞

    Args:
        phrase: –ü–æ–∏—Å–∫–æ–≤–∞—è —Ñ—Ä–∞–∑–∞
        minus_words: –°–ø–∏—Å–æ–∫ –º–∏–Ω—É—Å-—Å–ª–æ–≤

    Returns:
        bool: True –µ—Å–ª–∏ —Ñ—Ä–∞–∑–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
    """
    if not minus_words:
        return False

    phrase_lower = phrase.lower()
    for minus_word in minus_words:
        if minus_word.lower() in phrase_lower:
            return True
    return False


def categorize_phrase(phrase, city_name):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø–æ–∏—Å–∫–æ–≤–æ–π —Ñ—Ä–∞–∑—ã

    Args:
        phrase: –ü–æ–∏—Å–∫–æ–≤–∞—è —Ñ—Ä–∞–∑–∞
        city_name: –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    Returns:
        tuple: (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —ç–º–æ–¥–∑–∏)
    """
    phrase_lower = phrase.lower()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å
    is_local = city_name.lower() in phrase_lower

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: price > commercial > informational > comparison
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword in phrase_lower:
                emoji = CATEGORY_EMOJI.get(category, 'üîç')
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                if is_local and category != 'informational':
                    emoji = f"{CATEGORY_EMOJI['local']} {emoji}"
                return category.capitalize(), emoji

    # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–æ—à–ª–∞ –Ω–∏ –æ–¥–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è
    emoji = CATEGORY_EMOJI['local'] if is_local else CATEGORY_EMOJI['other']
    return 'Other', emoji


def track_duplicates(all_phrases):
    """
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ñ—Ä–∞–∑ –∏ —Å—á–∏—Ç–∞–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏—è

    Args:
        all_phrases: –°–ª–æ–≤–∞—Ä—å {—Ñ—Ä–∞–∑–∞: [—Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è]}

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
    """
    duplicates = {}

    for phrase, sources in all_phrases.items():
        if len(sources) > 1:
            duplicates[phrase] = len(sources)

    return duplicates


def format_number(num):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ —Ç—ã—Å—è—á"""
    return f"{num:,}".replace(',', ' ')


def generate_header(config, total_queries, timestamp):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —à–∞–ø–∫—É Markdown –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    business = config['business_info']
    settings = config['parser_settings']

    header = f"""# üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å –í–æ—Ä–¥—Å—Ç–∞—Ç

**–î–∞—Ç–∞:** {timestamp}
**–†–µ–≥–∏–æ–Ω:** {business['city']} ({business['region_code']})
**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:** {', '.join(settings['devices'])}
**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤:** {total_queries}

---

"""
    return header


def filter_phrases_by_minus_words(phrases, minus_words):
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ñ—Ä–∞–∑—ã –ø–æ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞–º

    Args:
        phrases: –°–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ –∏–∑ API
        minus_words: –°–ø–∏—Å–æ–∫ –º–∏–Ω—É—Å-—Å–ª–æ–≤

    Returns:
        tuple: (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã, —É–¥–∞–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã)
    """
    if not minus_words:
        return phrases, []

    filtered = []
    removed = []

    for phrase_data in phrases:
        phrase = phrase_data['phrase']
        if has_minus_words(phrase, minus_words):
            removed.append(phrase)
        else:
            filtered.append(phrase_data)

    return filtered, removed


def generate_query_section(query_num, query, result, city, limit, seen_phrases, minus_words=None):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ–∫—Ü–∏—é Markdown –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

    Args:
        query_num: –ù–æ–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        result: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç API
        city: –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
        limit: –õ–∏–º–∏—Ç —Ñ—Ä–∞–∑ –¥–ª—è –≤—ã–≤–æ–¥–∞
        seen_phrases: –°–ª–æ–≤–∞—Ä—å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        minus_words: –°–ø–∏—Å–æ–∫ –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

    Returns:
        str: Markdown —Å–µ–∫—Ü–∏—è
    """
    if not result or 'topRequests' not in result:
        return f"## üîç –ó–∞–ø—Ä–æ—Å {query_num}: {query}\n\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ\n\n---\n\n"

    total_count = result.get('totalCount', 0)
    phrases = result.get('topRequests', [])

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞–º
    filtered_phrases, removed_phrases = filter_phrases_by_minus_words(phrases, minus_words or [])

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
    phrases_sorted = sorted(filtered_phrases, key=lambda x: x['count'], reverse=True)
    top_phrases = phrases_sorted[:limit]

    removed_note = f"\n**–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞–º:** {len(removed_phrases)}" if removed_phrases else ""

    section = f"""## üîç –ó–∞–ø—Ä–æ—Å {query_num}: {query}

**–û–±—â–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å:** {format_number(total_count)} –ø–æ–∫–∞–∑–æ–≤/–º–µ—Å
**–ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–∑:** {len(phrases)}{removed_note}
**–¢–æ–ø-{limit} –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏:**

| ‚Ññ | –§—Ä–∞–∑–∞ | –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å | –¢–∏–ø |
|---|-------|-------------|-----|
"""

    for i, item in enumerate(top_phrases, 1):
        phrase_text = item['phrase']
        count = format_number(item['count'])
        category, emoji = categorize_phrase(phrase_text, city)

        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if phrase_text in seen_phrases:
            seen_phrases[phrase_text].append(query)
            duplicate_mark = f" *(–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {len(seen_phrases[phrase_text])} –∑–∞–ø—Ä–æ—Å–∞—Ö)*"
        else:
            seen_phrases[phrase_text] = [query]
            duplicate_mark = ""

        section += f"| {i} | {phrase_text}{duplicate_mark} | {count} | {emoji} {category} |\n"

    section += "\n---\n\n"
    return section


def generate_summary(all_results, seen_phrases):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã
    all_phrases_list = []
    for result in all_results:
        if result and 'topRequests' in result:
            all_phrases_list.extend(result['topRequests'])

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
    all_phrases_sorted = sorted(all_phrases_list, key=lambda x: x['count'], reverse=True)
    top_10 = all_phrases_sorted[:10]

    summary = """## üìà –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

**–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑ –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º:**

| ‚Ññ | –§—Ä–∞–∑–∞ | –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å |
|---|-------|-------------|
"""

    for i, item in enumerate(top_10, 1):
        phrase = item['phrase']
        count = format_number(item['count'])
        summary += f"| {i} | {phrase} | {count} |\n"

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates = {k: v for k, v in seen_phrases.items() if len(v) > 1}

    if duplicates:
        summary += f"\n**–§—Ä–∞–∑—ã, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö:** {len(duplicates)}\n"

    return summary


def save_results(config, queries, all_results, seen_phrases):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ results.md"""
    from datetime import datetime

    timestamp = datetime.now().strftime("%d.%m.%Y %H:%M")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    content = generate_header(config, len(queries), timestamp)

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–∫—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    limit = config['parser_settings']['top_results_limit']
    city = config['business_info']['city']
    minus_words = config['parser_settings'].get('minus_words', [])

    for i, (query, result) in enumerate(zip(queries, all_results), 1):
        section = generate_query_section(i, query, result, city, limit, seen_phrases, minus_words)
        content += section

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    content += generate_summary([r for r in all_results if r], seen_phrases)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open('results.md', 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results.md")


def collect_top_phrases_for_recursion(all_results, minus_words, top_n=10):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ø N —Ñ—Ä–∞–∑ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞

    Args:
        all_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
        minus_words: –°–ø–∏—Å–æ–∫ –º–∏–Ω—É—Å-—Å–ª–æ–≤
        top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ñ—Ä–∞–∑

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    all_phrases = []

    for result in all_results:
        if result and 'topRequests' in result:
            phrases = result['topRequests']
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞–º
            filtered, _ = filter_phrases_by_minus_words(phrases, minus_words)
            all_phrases.extend(filtered)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø N
    sorted_phrases = sorted(all_phrases, key=lambda x: x['count'], reverse=True)
    top_phrases = sorted_phrases[:top_n]

    return [p['phrase'] for p in top_phrases]


def save_to_csv(seen_phrases, minus_words):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã"""
    import csv

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã —Å –∏—Ö –¥–∞–Ω–Ω—ã–º–∏
    phrases_data = []
    for phrase, sources in seen_phrases.items():
        if not has_minus_words(phrase, minus_words):
            # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–æ–Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞)
            phrases_data.append({
                'phrase': phrase,
                'sources_count': len(sources)
            })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
    with open('results.csv', 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['–§—Ä–∞–∑–∞', '–í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö'])

        for data in phrases_data:
            writer.writerow([data['phrase'], data['sources_count']])

    print(f"üìä –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV: results.csv")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    from datetime import datetime

    start_time = datetime.now()

    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –Ø–Ω–¥–µ–∫—Å –í–æ—Ä–¥—Å—Ç–∞—Ç\n")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    token = load_env()
    config = load_config()
    queries = load_queries()

    business = config['business_info']
    settings = config['parser_settings']

    recursive_enabled = settings.get('recursive_parsing', False)
    recursion_depth = settings.get('recursion_depth', 2)
    recursive_top_n = settings.get('recursive_top_queries', 10)
    minus_words = settings.get('minus_words', [])

    print(f"üìÇ –†–µ–≥–∏–æ–Ω: {business['city']} ({business['region_code']})")
    print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {', '.join(settings['devices'])}")
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
    if recursive_enabled:
        print(f"üîÑ –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –í–ö–õ (–≥–ª—É–±–∏–Ω–∞: {recursion_depth}, —Ç–æ–ø: {recursive_top_n})")
    if minus_words:
        print(f"üö´ –ú–∏–Ω—É—Å-—Å–ª–æ–≤–∞: {len(minus_words)} —à—Ç.")
    print()

    # –•—Ä–∞–Ω–∏–ª–∏—â–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_results = []
    seen_phrases = {}
    all_queries = list(queries)  # –ö–æ–ø–∏—è –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–∏

    # –£–†–û–í–ï–ù–¨ 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞–∑–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    print(f"{'='*50}")
    print("üìç –£–†–û–í–ï–ù–¨ 1: –ë–∞–∑–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
    print(f"{'='*50}\n")

    for i, query in enumerate(queries, 1):
        print(f"[{i}/{len(queries)}] –ü–∞—Ä—Å–∏–Ω–≥: \"{query}\"")

        result = fetch_top_requests(
            token=token,
            phrase=query,
            region=business['region_code'],
            devices=settings['devices']
        )

        if result and 'topRequests' in result:
            total_count = result.get('totalCount', 0)
            phrases_count = len(result.get('topRequests', []))
            print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {phrases_count} —Ñ—Ä–∞–∑ ({format_number(total_count)} –ø–æ–∫–∞–∑–æ–≤/–º–µ—Å)")
        else:
            print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

        all_results.append(result)

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
        if i < len(queries):
            delay = settings['delay_between_requests']
            print(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫...\n")
            time.sleep(delay)
        else:
            print()

    # –£–†–û–í–ï–ù–¨ 2: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    if recursive_enabled and recursion_depth >= 2:
        print(f"\n{'='*50}")
        print("üìç –£–†–û–í–ï–ù–¨ 2: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ø —Ñ—Ä–∞–∑")
        print(f"{'='*50}\n")

        # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ø —Ñ—Ä–∞–∑—ã –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–∏
        recursive_queries = collect_top_phrases_for_recursion(all_results, minus_words, recursive_top_n)
        print(f"üéØ –û—Ç–æ–±—Ä–∞–Ω–æ {len(recursive_queries)} —Ñ—Ä–∞–∑ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞\n")

        for i, query in enumerate(recursive_queries, 1):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            if query in all_queries:
                continue

            all_queries.append(query)
            print(f"[{i}/{len(recursive_queries)}] –ü–∞—Ä—Å–∏–Ω–≥ (L2): \"{query}\"")

            result = fetch_top_requests(
                token=token,
                phrase=query,
                region=business['region_code'],
                devices=settings['devices']
            )

            if result and 'topRequests' in result:
                total_count = result.get('totalCount', 0)
                phrases_count = len(result.get('topRequests', []))
                print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {phrases_count} —Ñ—Ä–∞–∑ ({format_number(total_count)} –ø–æ–∫–∞–∑–æ–≤/–º–µ—Å)")
            else:
                print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

            all_results.append(result)

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(recursive_queries):
                delay = settings['delay_between_requests']
                print(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫...\n")
                time.sleep(delay)
            else:
                print()

    # –ü–æ–¥—Å—á—ë—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    successful = sum(1 for r in all_results if r)
    failed = len(all_queries) - successful

    print(f"{'='*50}")
    print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful}/{len(all_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    if failed > 0:
        print(f"‚ö†Ô∏è  –ù–µ—É–¥–∞—á–Ω—ã—Ö: {failed}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if successful > 0:
        save_results(config, all_queries, all_results, seen_phrases)

        # –ü–æ–¥—Å—á—ë—Ç –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑
        unique_phrases = len(seen_phrases)
        print(f"üìù –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑: {unique_phrases}")

        # –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
        save_to_csv(seen_phrases, minus_words)
    else:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")

    # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.1f} —Å–µ–∫")
    print(f"\n{'='*50}")
    print("üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω!")


if __name__ == "__main__":
    main()
